# Pyspark-interview-
most asked interview questons
Important Interview Question On Spark
=========================================
1. Difference between RDD & Dataframes
2. What are the challenges you face in spark?
3. What is difference between reduceByKey & groupByKey?
4. What is the difference between Persist and Cache?
5. What is the Advantage of a Parquet File?
6. What is a Broadcast Join ?
7. What is Difference between Coalesce and Repartition?
8. What are the roles and responsibility of driver in spark Architecture?
9. What is meant by Data Skewness? How is it deal? 
10. What are the optimisation techniques used in Spark?
11. What is Difference Between Map and FlatMap?
12. What are accumulator and BroadCast Variables?
13. What is a OOM Issue, how to deal it?
14. what are tranformation in spark? Type of Transformation?
15. Tell me some action in spark that you used ?
16. What is the role of Catalyst Optimizer ?
17. what is the checkpointing?
18. Cache and persist
19. What do you understand by Lazy Evaluation ?
20. How to convert Rdd to Dataframe?
21. How to Dataframe to Dataset.
22. What makes Spark better than Hadoop?
23. How can you read a CSV file without using an external schema?
24. What is the difference between Narrow Transformation and Wide Transformation?
25. What are the different parameters that can be passed while Spark-submit?
26. What are Global Temp View and Temp View?
27. How can you add two new columns to a Data frame with some calculated values?
28. Avro Vs ORC, which one do you prefer?
29. What are the different types of joins in Spark?
30. Can you explain Anti join and Semi join?
31. What is the difference between Order By, Sort By, and Cluster By?
32. Data Frame vs Dataset in spark?
33. 4.What are the join strategies in Spark
34. What happens in Cluster deployment mode and Client deployment mode
35. What are the parameters you have used in spark-submit
36. How do you add a new column in Spark
37. How do you drop a column in Spark
38. What is difference between map and flatmap?
39. What is skew partitions?
40. What is DAG and Lineage in Spark?
41. What is the difference between RDD and Dataframe?
42. Where we can find the spark application logs.
43. What is the difference between reduceByKey and groupByKey?
44. what is spark optimization?
45. What are shared variables in spark
46. What is a broadcast variable
47. Why spark instead of Hive
48. what is cache
49. Tell me the steps to read a file in spark
50. How do you handle 10 GB file in spark, how do you optimize it


PySpark Data Engineer Interview experience from someone on LinkedIn and want to share here for all my #connections

ğˆğ§ğ­ğ«ğ¨ğğ®ğœğ­ğ¢ğ¨ğ§:
1. Can you provide an overview of your experience working with PySpark and big data processing?
2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?

ğğ²ğ’ğ©ğšğ«ğ¤ ğğšğ¬ğ¢ğœğ¬:
3. Explain the basic architecture of PySpark.
4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?

ğƒğšğ­ğšğ…ğ«ğšğ¦ğ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğ¬:
5. Describe the difference between a DataFrame and an RDD in PySpark.
6. Can you explain transformations and actions in PySpark DataFrames?
7. Provide examples of PySpark DataFrame operations you frequently use.

ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğ¢ğ§ğ  ğğ²ğ’ğ©ğšğ«ğ¤ ğ‰ğ¨ğ›ğ¬:
8. How do you optimize the performance of PySpark jobs?
9. Can you discuss techniques for handling skewed data in PySpark?

ğƒğšğ­ğš ğ’ğğ«ğ¢ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ‚ğ¨ğ¦ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§:
10. Explain how data serialization works in PySpark.
11. Discuss the significance of choosing the right compression codec for your PySpark applications.

ğ‡ğšğ§ğğ¥ğ¢ğ§ğ  ğŒğ¢ğ¬ğ¬ğ¢ğ§ğ  ğƒğšğ­ğš:
12. How do you deal with missing or null values in PySpark DataFrames?
13. Are there any specific strategies or functions you prefer for handling missing data?

ğ–ğ¨ğ«ğ¤ğ¢ğ§ğ  ğ°ğ¢ğ­ğ¡ ğğ²ğ’ğ©ğšğ«ğ¤ ğ’ğğ‹:
14. Describe your experience with PySpark SQL.
15. How do you execute SQL queries on PySpark DataFrames?

ğğ«ğ¨ğšğğœğšğ¬ğ­ğ¢ğ§ğ  ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤:
16. What is broadcasting, and how is it useful in PySpark?
17. Provide an example scenario where broadcasting can significantly improve performance.

ğğ²ğ’ğ©ğšğ«ğ¤ ğŒğšğœğ¡ğ¢ğ§ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ :
18. Discuss your experience with PySpark's MLlib.
19. Can you give examples of machine learning algorithms you've implemented using PySpark?

ğ‰ğ¨ğ› ğŒğ¨ğ§ğ¢ğ­ğ¨ğ«ğ¢ğ§ğ  ğšğ§ğ ğ‹ğ¨ğ ğ ğ¢ğ§ğ :
20. How do you monitor and troubleshoot PySpark jobs?
21. Describe the importance of logging in PySpark applications.

ğˆğ§ğ­ğğ ğ«ğšğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğğ­ğ¡ğğ« ğ“ğğœğ¡ğ§ğ¨ğ¥ğ¨ğ ğ¢ğğ¬:
22. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.
23. How do you handle data transfer between PySpark and external systems?

ğ‘ğğšğ¥-ğ°ğ¨ğ«ğ¥ğ ğğ«ğ¨ğ£ğğœğ­ ğ’ğœğğ§ğšğ«ğ¢ğ¨:
24. Explain the project that you worked on in your previous organizations.
25. Describe a challenging PySpark project you've worked on. What were the key challenges, and how did you overcome them?

ğ‚ğ¥ğ®ğ¬ğ­ğğ« ğŒğšğ§ğšğ ğğ¦ğğ§ğ­:
26. Explain your experience with cluster management in PySpark.
27. How do you scale PySpark applications in a cluster environment?

ğğ²ğ’ğ©ğšğ«ğ¤ ğ„ğœğ¨ğ¬ğ²ğ¬ğ­ğğ¦:
28. Can you name and briefly describe some popular libraries or tools in the PySpark ecosystem, apart from the core PySpark functionality?
